{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j12YgYtdP9JK"
      },
      "source": [
        "# üï∏Ô∏è Scrapify\n",
        "\n",
        "<!-- [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/YonaniCodes/Scrapify/blob/main/notebooks/scrapify-demo.ipynb) -->\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def clean_widget_metadata(nb_path, output_path=None):\n",
        "    with open(nb_path, 'r', encoding='utf-8') as f:\n",
        "        notebook = json.load(f)\n",
        "\n",
        "    if 'widgets' in notebook.get('metadata', {}):\n",
        "        del notebook['metadata']['widgets']\n",
        "\n",
        "    output_path = output_path or nb_path\n",
        "    with open(output_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(notebook, f, indent=2)\n",
        "\n",
        "# Example usage: download your notebook first, then clean and fix it\n",
        "# clean_widget_metadata('/content/your_notebook.ipynb')\n"
      ],
      "metadata": {
        "id": "JlQydWaGeQqr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "!ls\n",
        "# !jupyter nbconvert --to html scrapify_demo.ipynb\n"
      ],
      "metadata": {
        "id": "1_NV8jgteWoL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WFTPeEs3P9JN",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "# Verify we're running in Colab - exit if not\n",
        "try:\n",
        "    import google.colab\n",
        "except ImportError:\n",
        "    raise RuntimeError(\"This script is designed to run only in Google Colab\")\n",
        "\n",
        "# Define repo details\n",
        "repo_url = \"https://github.com/YonaniCodes/Scrapify.git\"\n",
        "repo_path = \"/content/Scrapify\"\n",
        "\n",
        "# Clone or update the repository\n",
        "if not os.path.exists(repo_path):\n",
        "    !git clone {repo_url} {repo_path}\n",
        "else:\n",
        "    %cd {repo_path}\n",
        "    !git stash  # Stash any local changes\n",
        "    !git pull\n",
        "\n",
        "# Add src/ to Python path\n",
        "sys.path.append(f\"{repo_path}/src\")\n",
        "\n",
        "# Import the scraper function (replace with actual import)\n",
        "# from scraper_module import scrape_function\n",
        "!pip install -r requirements.txt\n",
        "!pip install PyMuPDF\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fSzH6oRxeVlS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mb06iS-9l99j"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "\n",
        "# Get output of ls command\n",
        "output = subprocess.check_output(\"ls\", shell=True).decode().splitlines()\n",
        "\n",
        "# Now you can conditionally check\n",
        "if \"firebase-adminsdk.json\" in output:\n",
        "    print(\"Everything is ready you can start working üéâüéâüéâüéâ\")\n",
        "else:\n",
        "    print(\"firebase-adminsdk.json not found.üòíüòíüòí please ask Yonani for the file\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oaUyoeqXm6hX"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('/content/Scrapify/src')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scrapify import scrape\n",
        "from scrapify import get_report\n",
        "from preprocessing import normalize_amharic\n",
        "\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "S2FG4Do4UihW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Steps to Scrape a website\n",
        "\n",
        "1. Install the  `googlesearch-python` module\n",
        "2. Define a function that takes two arguments `num_urls` and `search_query`. and return list of urls\n",
        "3. call the `scrape` method by passing list of `urls` and `your_name` as a second paramete (optional)."
      ],
      "metadata": {
        "id": "M06ZSYp4CZ7D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install googlesearch-python"
      ],
      "metadata": {
        "id": "xn35piLXV9CZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from googlesearch import search\n",
        "\n",
        "def get_links_from_simple_search(query, num_results):\n",
        "    \"\"\"\n",
        "    Retrieves URLs from a simple Google search.\n",
        "\n",
        "    Args:\n",
        "        query (str): The search query.\n",
        "        num_results (int): The desired number of search results. This is not directly supported by googlesearch,\n",
        "                          so it will be used to limit the results after fetching.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of URLs.\n",
        "    \"\"\"\n",
        "    # Fetch the search results\n",
        "    search_results = search(query, num_results=num_results)\n",
        "    # Convert the generator object to a list\n",
        "    urls = list(set(search_results))\n",
        "    return urls"
      ],
      "metadata": {
        "id": "5MFk1o0VdjyO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need a function to filter if the url is already scraped we dont wanna send reques to our backend everyurl that is returned by the function `get_links_from_simple_search`"
      ],
      "metadata": {
        "id": "EqbPfI4oEVAN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_url(urls):\n",
        "# Assuming scraped_data and unscraped_data are pandas DataFrames with a 'url' column\n",
        "  scraped,unscrape=get_report()\n",
        "  scraped_data= pd.DataFrame(scraped)\n",
        "  unscraped_data= pd.DataFrame(unscrape)\n",
        "  filtered_urls = []\n",
        "  for url in urls:\n",
        "      # Check if the URL is NOT in both scraped_data.url and unscraped_data.url\n",
        "      if url not in scraped_data['url'].values and url not in unscraped_data['url'].values:\n",
        "          filtered_urls.append(url)\n",
        "  return filtered_urls"
      ],
      "metadata": {
        "id": "WMG-PyJyEzZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lets visualize our scraping status**"
      ],
      "metadata": {
        "id": "sYavEXkBkCsB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "def plot_scraping_report():\n",
        "    \"\"\"\n",
        "    Generate a pie chart showing the ratio of scraped to unscraped URLs.\n",
        "    Uses get_report() internally to fetch the data.\n",
        "    \"\"\"\n",
        "    # Get the data\n",
        "    scraped, unscraped = get_report()\n",
        "\n",
        "    # Convert to DataFrames\n",
        "    scraped_data = pd.DataFrame(scraped)\n",
        "    unscraped_data = pd.DataFrame(unscraped)\n",
        "\n",
        "    # Calculate counts\n",
        "    scraped_count = len(scraped_data['url'])\n",
        "    unscraped_count = len(unscraped_data['url'])\n",
        "\n",
        "    # Prepare data for visualization\n",
        "    labels = ['Scraped URLs', 'Unscraped URLs']\n",
        "    sizes = [scraped_count, unscraped_count]\n",
        "    colors = ['salmon', 'lightblue']  # Green and red\n",
        "\n",
        "    # Create the figure\n",
        "    fig, ax = plt.subplots(figsize=(6, 6))\n",
        "    ax.pie(sizes,\n",
        "           labels=labels,\n",
        "           colors=colors,\n",
        "           autopct=lambda p: f'{p:.1f}%\\n({int(p/100*sum(sizes))})',\n",
        "           startangle=90,\n",
        "           wedgeprops={'linewidth': 1, 'edgecolor': 'white'},\n",
        "           textprops={'fontsize': 12})\n",
        "\n",
        "    ax.set_title('Scraping Report: Scraped vs Unscraped URLs', pad=20, fontsize=14)\n",
        "    plt.tight_layout()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "g7Y_fbaDhvqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage:\n",
        "plot_scraping_report()"
      ],
      "metadata": {
        "id": "GotGbA9_HMfT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Normalizing our data\n",
        "\n",
        "\n",
        "In order to normalize our well define\n",
        "1. deffine a function `read_jsonl_content` to read jsoln file\n",
        "2. normalize the text returned using `normalize_and_save_data`\n",
        "3. Finall define the wraper function `normalize_jsonl_file`\n",
        "\n"
      ],
      "metadata": {
        "id": "rKge09SIHms3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def read_jsonl_content(file_path, content_key='content'):\n",
        "    \"\"\"Returns a list of 'content' values from a .jsonl file.\"\"\"\n",
        "    content_values = []\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        for line in file:\n",
        "            stripped_line = line.strip()\n",
        "            if stripped_line:\n",
        "                record = json.loads(stripped_line)\n",
        "                content_values.append(record[content_key])\n",
        "\n",
        "    print(f\"Read {len(content_values)} records from {file_path}.\")\n",
        "    return content_values  # Example: [\"text1\", \"text2\", ...]"
      ],
      "metadata": {
        "id": "GkkamYUqI2q6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_and_save_data(jsonl_data, file_path):\n",
        "    \"\"\"Writes all JSONL entries as one continuous text blob.\"\"\"\n",
        "    with open(file_path, 'w', encoding='utf-8') as f:\n",
        "        all_text = \" \".join(normalize_amharic(text) for text in jsonl_data)  # No newlines\n",
        "        f.write(all_text)"
      ],
      "metadata": {
        "id": "MCszEdIzKC1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_jsonl_file(input_file, output_file):\n",
        "  jsonl_data=read_jsonl_content(test_file_path)\n",
        "  normalize_and_save_data(jsonl_data, destination_file_path)"
      ],
      "metadata": {
        "id": "JIbdpNkNQRhQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How to normalize data?\n",
        "\n",
        "1. define `test_file_path` the jsonl file is found.\n",
        "2. define `destination` you want to store the normaliez text file\n",
        "3. call the function `normalize_and_save_data  ` with the two parameters\n",
        "4. finnally call the wraper function"
      ],
      "metadata": {
        "id": "bq2V7Peg7jPW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_file_path = '/content/Scrapify/extracted_data.jsonl'\n",
        "destination_file_path = '/content/Scrapify/normalized.txt'\n",
        "\n",
        "normalize_jsonl_file(test_file_path, destination_file_path)\n"
      ],
      "metadata": {
        "id": "im0UOg_aQcuo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chunking the data. now is the time to chunk all the data."
      ],
      "metadata": {
        "id": "kbMM-a6hRAgn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import os\n",
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "def read_normalized_text(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        return f.read()\n",
        "\n",
        "\n",
        "def chunk_text(text, chunk_size=1000, chunk_overlap=20):\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    while start < len(text):\n",
        "        end = start + chunk_size\n",
        "        chunks.append(text[start:end])\n",
        "        start = end - chunk_overlap\n",
        "    return chunks\n",
        "\n",
        "\n",
        "def save_chunks_as_files(chunks: list[str], output_dir: str, base_filename: str = \"chunk\"):\n",
        "    \"\"\"\n",
        "    Saves each text chunk as an individual .txt file.\n",
        "\n",
        "    Args:\n",
        "        chunks: List of text chunks (strings)\n",
        "        output_dir: Directory to save the files\n",
        "        base_filename: Prefix for filenames (default: \"chunk\")\n",
        "\n",
        "    Returns:\n",
        "        List of paths to the created files\n",
        "    \"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)  # Create dir if it doesn't exist\n",
        "    saved_paths = []\n",
        "\n",
        "    for i, chunk in enumerate(chunks, start=1):\n",
        "        # Format: \"{output_dir}/chunk_1.txt\", \"{output_dir}/chunk_2.txt\", etc.\n",
        "        file_path = os.path.join(output_dir, f\"{base_filename}_{i}.txt\")\n",
        "\n",
        "        with open(file_path, 'w', encoding='utf-8') as file:\n",
        "            file.write(chunk)\n",
        "        saved_paths.append(file_path)\n",
        "\n",
        "    print(f\"Saved {len(saved_paths)} files:\")\n",
        "    for path in saved_paths[:10]:\n",
        "      print(f\"‚Üí {path}\")\n",
        "\n",
        "\n",
        "def zip_chunks_folder(folder_path):\n",
        "    zip_file_path = f\"{folder_path}.zip\"\n",
        "    shutil.make_archive(folder_path, 'zip', folder_path)\n",
        "    return zip_file_path\n",
        "\n"
      ],
      "metadata": {
        "id": "l03Kn6k2dNal"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "normalized_file_path = '/content/Scrapify/normalized.txt'\n",
        "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "destination_file_path = f'/content/Scrapify/chunks{timestamp}'\n",
        "text=read_normalized_text(normalized_file_path)\n",
        "chunks=chunk_text(text)\n",
        "save_chunks_as_files(chunks,destination_file_path)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wR0rRQ1Sd76n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Downloading the chunks"
      ],
      "metadata": {
        "id": "AvxWQLLBf8A3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "zip_file_path = zip_chunks_folder(destination_file_path)\n",
        "files.download(zip_file_path)\n",
        "\n",
        "print(f\"Chunks folder zipped and ready for download: {zip_file_path}\")\n"
      ],
      "metadata": {
        "id": "kH2O-LC--cFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Uploading chunks"
      ],
      "metadata": {
        "id": "Gj5qzwYEAtVU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: a function which uploads chunck zip file and unzip it and return array of chunks\n",
        "\n",
        "import zipfile\n",
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "def upload_and_unzip_chunks(zip_file_path):\n",
        "    \"\"\"\n",
        "    Uploads a zip file containing text chunks, unzips it, and returns a list of chunk file paths.\n",
        "\n",
        "    Args:\n",
        "        zip_file_path: The path to the uploaded zip file.\n",
        "\n",
        "    Returns:\n",
        "        A list of strings, where each string is the path to an unzipped chunk file.\n",
        "        Returns an empty list if the upload or unzipping fails.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        uploaded = files.upload()\n",
        "        if zip_file_path not in uploaded:\n",
        "            print(f\"Error: File '{zip_file_path}' not found in the uploaded files.\")\n",
        "            return []\n",
        "\n",
        "        with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall('/tmp/chunks')  # Extract to a temporary directory\n",
        "\n",
        "        chunk_files = []\n",
        "        for filename in os.listdir('/tmp/chunks'):\n",
        "            if filename.endswith(\".txt\"):  # Assuming chunks are text files\n",
        "                chunk_files.append(os.path.join('/tmp/chunks', filename))\n",
        "        return chunk_files\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        return []\n"
      ],
      "metadata": {
        "id": "bivIJ85VAsbH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code to generate vector embedding"
      ],
      "metadata": {
        "id": "IbY0fF-v-doJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install sentence-transformers if you don't have it installed already\n",
        "# pip install sentence-transformers\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Load the multilingual SBERT model\n",
        "model = SentenceTransformer('paraphrase-xlm-r-multilingual-v1')\n",
        "\n",
        "# Sample text chunks (this could be sentences or paragraphs from your text)\n",
        "chunks = [\n",
        "    \"The cat jumped over the fence.\",\n",
        "    \"A dog is playing in the park.\",\n",
        "    \"Elephants are the largest land animals.\",\n",
        "    \"Cats are often playful and curious.\"\n",
        "]\n",
        "\n",
        "# Generate embeddings for the chunks\n",
        "vectors = model.encode(chunks, show_progress_bar=True)\n"
      ],
      "metadata": {
        "id": "16AQBrovIcUk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Similarity Check"
      ],
      "metadata": {
        "id": "Y3SU0yw7IgA0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Compute cosine similarity between all embeddings\n",
        "similarities = cosine_similarity(vectors)\n",
        "\n",
        "# Example: Checking similarity between the first chunk and the second chunk\n",
        "print(f\"Cosine similarity between chunk 0 and chunk 1: {similarities[0][1]}\")\n",
        "\n",
        "# Example: Checking similarity between the first chunk and the third chunk\n",
        "print(f\"Cosine similarity between chunk 0 and chunk 2: {similarities[0][2]}\")\n"
      ],
      "metadata": {
        "id": "seSnOMvqI0uf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# it works for me"
      ],
      "metadata": {
        "id": "MKzSbY1GJBXO"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}