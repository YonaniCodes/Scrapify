{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0390888-2043-4d6b-9ebf-ecde8cd8d951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\programdata\\anaconda3\\lib\\site-packages (4.12.3)\n",
      "Requirement already satisfied: pymupdf in c:\\users\\professor ab\\appdata\\roaming\\python\\python312\\site-packages (1.25.5)\n",
      "Requirement already satisfied: pytesseract in c:\\users\\professor ab\\appdata\\roaming\\python\\python312\\site-packages (0.3.13)\n",
      "Requirement already satisfied: pdf2image in c:\\users\\professor ab\\appdata\\roaming\\python\\python312\\site-packages (1.17.0)\n",
      "Requirement already satisfied: Pillow in c:\\programdata\\anaconda3\\lib\\site-packages (10.4.0)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\professor ab\\appdata\\roaming\\python\\python312\\site-packages (4.11.0.86)\n",
      "Requirement already satisfied: langdetect in c:\\users\\professor ab\\appdata\\roaming\\python\\python312\\site-packages (1.0.9)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (2024.8.30)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from pytesseract) (24.1)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\lib\\site-packages (from langdetect) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install requests beautifulsoup4 pymupdf pytesseract pdf2image Pillow opencv-python langdetect numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25e27c0a-3973-4e35-a6d6-bcc127127995",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import fitz  # PyMuPDF\n",
    "import pytesseract\n",
    "from pdf2image import convert_from_path\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "from langdetect import detect\n",
    "import time\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "pytesseract.pytesseract.tesseract_cmd = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca37a833-8b29-4d45-a238-4ac361ab84b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Configuration\n",
    "BASE_URLS = [\n",
    "    \"https://www.lawethiopia.com\",  # Replace with actual legal sites\n",
    "    \"https://www.fanabc.com\",\n",
    "    \"https://borkena.com/category/law/\"\n",
    "]\n",
    "DOWNLOAD_DIR = \"amharic_legal_pdfs\"\n",
    "OUTPUT_JSON = \"amharic_legal_data.json\"\n",
    "USER_AGENT = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) LegalResearchBot/1.0\"\n",
    "REQUEST_DELAY = 5  # seconds between requests\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(DOWNLOAD_DIR, exist_ok=True)\n",
    "\n",
    "def preprocess_image(image):\n",
    "    \"\"\"Enhance image quality for better OCR results\"\"\"\n",
    "    # Convert to numpy array\n",
    "    img = np.array(image)\n",
    "    \n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "    # Apply adaptive thresholding\n",
    "    thresh = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "                                  cv2.THRESH_BINARY, 11, 2)\n",
    "    \n",
    "    # Denoising\n",
    "    denoised = cv2.fastNlMeansDenoising(thresh, None, 30, 7, 21)\n",
    "    \n",
    "    return Image.fromarray(denoised)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d5de6fe-3879-4bd3-ba48-1194d0f02cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_with_ocr(pdf_path):\n",
    "    \"\"\"Extract text from scanned PDFs using OCR\"\"\"\n",
    "    try:\n",
    "        images = convert_from_path(pdf_path, dpi=300)\n",
    "        text = \"\"\n",
    "        \n",
    "        for i, image in enumerate(images):\n",
    "            # Pre-process image\n",
    "            processed_image = preprocess_image(image)\n",
    "            \n",
    "            # OCR with Amharic language\n",
    "            page_text = pytesseract.image_to_string(processed_image, lang='amh')\n",
    "            \n",
    "            # Post-process text\n",
    "            cleaned_text = re.sub(r'\\s+', ' ', page_text).strip()\n",
    "            text += cleaned_text + \"\\n\"\n",
    "        \n",
    "        return text if text.strip() else None\n",
    "    except Exception as e:\n",
    "        print(f\"OCR failed for {pdf_path}: {str(e)}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26d7eed6-ae9c-4d04-8bf7-540afd396ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_text_from_pdf(pdf_url):\n",
    "    \"\"\"Hybrid text extraction - tries digital first, then OCR\"\"\"\n",
    "    try:\n",
    "        # Download PDF\n",
    "        response = requests.get(pdf_url, headers={'User-Agent': USER_AGENT})\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # First try digital text extraction\n",
    "        with fitz.open(stream=response.content, filetype=\"pdf\") as doc:\n",
    "            digital_text = \"\"\n",
    "            for page in doc:\n",
    "                digital_text += page.get_text(\"text\") or \"\"\n",
    "        \n",
    "        # If we got sufficient Amharic text, return it\n",
    "        if is_valid_amharic(digital_text):\n",
    "            return digital_text\n",
    "        \n",
    "        # Otherwise, save to temp file and try OCR\n",
    "        temp_path = os.path.join(DOWNLOAD_DIR, \"temp.pdf\")\n",
    "        with open(temp_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        \n",
    "        ocr_text = extract_text_with_ocr(temp_path)\n",
    "        os.remove(temp_path)  # Clean up\n",
    "        \n",
    "        return ocr_text\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {pdf_url}: {str(e)}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6d1321c-6bee-48fa-84d9-d3afa1239206",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def is_valid_amharic(text):\n",
    "    \"\"\"Enhanced validation for Amharic content\"\"\"\n",
    "    if not text or len(text.strip()) < 100:\n",
    "        return False\n",
    "    \n",
    "    # Check for minimum Amharic characters\n",
    "    amharic_chars = re.findall(r'[\\u1200-\\u137F]', text)\n",
    "    if len(amharic_chars) < 50:  # At least 50 Amharic characters\n",
    "        return False\n",
    "    \n",
    "    # Language detection (with fallback)\n",
    "    try:\n",
    "        return detect(text) == 'am'\n",
    "    except:\n",
    "        return bool(amharic_chars)  # Fallback to character check if detection fails\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb91443f-81b1-405b-9297-77a37a441776",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scrape_legal_pdfs():\n",
    "    \"\"\"Main scraping function with ethical delays and error handling\"\"\"\n",
    "    processed_urls = set()\n",
    "    results = []\n",
    "    \n",
    "    # Load existing results to avoid re-processing\n",
    "    if os.path.exists(OUTPUT_JSON):\n",
    "        with open(OUTPUT_JSON, 'r', encoding='utf-8') as f:\n",
    "            results = json.load(f)\n",
    "            processed_urls = {item['pdf_url'] for item in results}\n",
    "    \n",
    "    for base_url in BASE_URLS:\n",
    "        try:\n",
    "            print(f\"Processing: {base_url}\")\n",
    "            time.sleep(REQUEST_DELAY)\n",
    "            \n",
    "            response = requests.get(base_url, headers={'User-Agent': USER_AGENT})\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            for link in soup.find_all('a', href=True):\n",
    "                pdf_url = link['href']\n",
    "                \n",
    "                # Normalize URL\n",
    "                if not pdf_url.startswith('http'):\n",
    "                    pdf_url = requests.compat.urljoin(base_url, pdf_url)\n",
    "                \n",
    "                # Skip non-PDF or already processed\n",
    "                if not pdf_url.lower().endswith('.pdf') or pdf_url in processed_urls:\n",
    "                    continue\n",
    "                \n",
    "                print(f\"Found PDF: {pdf_url}\")\n",
    "                time.sleep(REQUEST_DELAY)\n",
    "                \n",
    "                # Extract text\n",
    "                text = extract_text_from_pdf(pdf_url)\n",
    "                if not text or not is_valid_amharic(text):\n",
    "                    continue\n",
    "                \n",
    "                # Save metadata\n",
    "                filename = os.path.basename(pdf_url)\n",
    "                filepath = os.path.join(DOWNLOAD_DIR, filename)\n",
    "                \n",
    "                # Save the PDF\n",
    "                with open(filepath, 'wb') as f:\n",
    "                    f.write(requests.get(pdf_url).content)\n",
    "                \n",
    "                # Add to results\n",
    "                result = {\n",
    "                    \"source\": base_url,\n",
    "                    \"pdf_url\": pdf_url,\n",
    "                    \"local_path\": filepath,\n",
    "                    \"title\": filename.replace('.pdf', ''),\n",
    "                    \"type\": \"legal_document\",\n",
    "                    \"language\": \"am\",\n",
    "                    \"content\": text,\n",
    "                    \"date_scraped\": time.strftime(\"%Y-%m-%d\")\n",
    "                }\n",
    "                results.append(result)\n",
    "                processed_urls.add(pdf_url)\n",
    "                \n",
    "                # Save incremental results\n",
    "                with open(OUTPUT_JSON, 'w', encoding='utf-8') as f:\n",
    "                    json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {base_url}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3377ca29-6ecb-45ba-81fd-f09e484c60c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Amharic legal document scraping...\n",
      "Processing: https://www.lawethiopia.com\n",
      "Found PDF: https://www.lawethiopia.com/images/cassation/Ethiopia cassation index volume 1-18.pdf\n",
      "Found PDF: https://www.lawethiopia.com/images/cassation/cassation decisions by volumes/volume 1-3.pdf\n",
      "Found PDF: https://www.lawethiopia.com/images/cassation/cassation decisions by volumes/volume 4.pdf\n",
      "OCR failed for amharic_legal_pdfs\\temp.pdf: Unable to get page count. Is poppler installed and in PATH?\n",
      "Found PDF: https://www.lawethiopia.com/images/cassation/cassation decisions by volumes/volume 5.pdf\n",
      "OCR failed for amharic_legal_pdfs\\temp.pdf: Unable to get page count. Is poppler installed and in PATH?\n",
      "Found PDF: https://www.lawethiopia.com/images/cassation/cassation decisions by volumes/volume 6.pdf\n",
      "OCR failed for amharic_legal_pdfs\\temp.pdf: Unable to get page count. Is poppler installed and in PATH?\n",
      "Found PDF: https://www.lawethiopia.com/images/cassation/cassation decisions by volumes/volume 7.pdf\n",
      "OCR failed for amharic_legal_pdfs\\temp.pdf: Unable to get page count. Is poppler installed and in PATH?\n",
      "Found PDF: https://www.lawethiopia.com/images/cassation/cassation decisions by volumes/volume 8.pdf\n",
      "Found PDF: https://www.lawethiopia.com/images/cassation/cassation decisions by volumes/volume 9.pdf\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Verify Tesseract is properly configured\n",
    "    try:\n",
    "        pytesseract.get_tesseract_version()\n",
    "    except:\n",
    "        print(\"Error: Tesseract OCR not properly installed or configured\")\n",
    "        print(\"On Windows: Download Amharic traineddata from https://github.com/tesseract-ocr/tessdata\")\n",
    "        exit(1)\n",
    "    \n",
    "    print(\"Starting Amharic legal document scraping...\")\n",
    "    final_results = scrape_legal_pdfs()\n",
    "    print(f\"Completed! Found {len(final_results)} valid Amharic legal documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b242880-32f3-4475-8001-6fe25d30c6b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pdf_env)",
   "language": "python",
   "name": "pdf_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
